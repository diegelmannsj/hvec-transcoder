#!/usr/bin/env python3

"""
Script to audit the loudness of all video files in a directory or a single file.

This script recursively scans a directory for movie files (mkv, mp4, avi),
runs a 'Pass 1' ffmpeg loudnorm analysis on each, and logs the results.

It is designed to be resumable; it reads its own log file on startup
to skip any files that have already been successfully processed.

v2.9.4: feat: Smart Normalization Detection
      - Enhanced Single File Mode: If a file is found in the log with a loudness
        matching the target (within 0.5 LUFS), it is explicitly marked as
        [NORMALIZED].
      - The script will display the cached stats (Loudness, Gain) and the
        timestamp of the log entry.
      - Crucially, it will SKIP the interactive normalization prompt if the
        file is already normalized, preventing redundant "Apply normalization?"
        questions for files that were just processed.

v2.9.3: feat: Automatic Fallback for Complex Audio
      - Defaults to strict v2.6.0 command, falls back to compatibility mode
        if "Invalid Argument" crash occurs.

v2.9.2: fix: Re-apply FFmpeg Crash Fix & Improve Log Matching
v2.9.1: feat: Neutral Wording for Normalization
v2.9.0: fix: Disable Smart Resume (Size Check)
v2.8.1: fix: Prioritize Success in Log
v2.8.0: feat: Verbose Re-scan Logic
v2.7.0: feat: Interactive Countdown Timer
v2.6.0: feat: Interactive Normalization
v2.4.0: feat: Colored Stats Summary
v2.3.0: feat: Auto-Background Priority
v2.2.0: feat: Output calculated gain adjustment to console
v2.1.0: feat: Use fast `ebur128` filter
v2.0.0: feat: Output log as a standard CSV file
"""

import argparse
import subprocess
import os
import sys
import json
import time
import logging
import re
import math
import multiprocessing
import csv
import shutil
import select
from tqdm import tqdm
from typing import Set, Tuple, List, Dict, Any, Optional

# --- Configuration ---
__version__ = "2.9.4"
VIDEO_EXTENSIONS = ('.mkv', '.mp4', '.avi')
DEFAULT_LOG_DIR = os.path.expanduser('~/.hvec')
DEFAULT_LOG_FILENAME = os.path.join(DEFAULT_LOG_DIR, 'audit_loudness.csv')
TARGET_LOUDNESS = -23.0
DEFAULT_WORKERS = 4
PROMPT_TIMEOUT_SECONDS = 25  # Seconds to wait for user input before defaulting to 'No'

# --- ANSI Colors ---
GREEN = "\033[92m"
RED = "\033[91m"
YELLOW = "\033[93m"
RESET = "\033[0m"

# --- Log header is now a list for the csv writer ---
LOG_HEADER = [
    "Timestamp", "Status", "File", "Size", "ScanTime",
    "CurrentLoudness", "Adjustment_dB", "MeasuredTP", "MeasuredLRA",
    "MeasuredThresh", "Error"
]

# --- PRIORITY CONTROL (Python Native) ---
def enforce_background_priority():
    if '--no-nice' in sys.argv:
        return
    if not sys.platform.startswith('linux'):
        return
    try:
        current_nice = os.nice(0)
        if current_nice < 19:
            print("Auto-adjusting priority: Switching to Background Mode (Nice 19, IO Idle)...")
            sys.stdout.flush()
            new_args = ['nice', '-n', '19', 'ionice', '-c', '3'] + sys.argv
            os.execvp('nice', new_args)
    except Exception as e:
        print(f"Warning: Could not auto-adjust priority: {e}", file=sys.stderr)

enforce_background_priority()


# --- Helper Function (from ,hvec) ---
def format_size(size_bytes):
    if size_bytes is None or not isinstance(size_bytes, (int, float)) or size_bytes < 0: return "N/A"
    if size_bytes == 0: return "0 B"
    size_name = ("B", "KiB", "MiB", "GiB", "TiB", "PiB", "EiB")
    try:
        i = int(math.floor(math.log(size_bytes, 1024)));
        i = min(i, len(size_name) - 1)
        p = math.pow(1024, i);
        s = round(size_bytes / p, 2)
        return f"{s} {size_name[i]}"
    except (ValueError, TypeError): return "N/A"


def load_processed_files(log_file: str, logger: logging.Logger) -> Tuple[Dict[str, Dict[str, Any]], Dict[str, Dict[str, Any]]]:
    """
    Reads the CSV data log file to find files already processed.
    Returns:
        processed_map: Dict mapping absolute file paths to their row data.
        filename_map: Dict mapping filenames (basename) to their row data (fallback).
    """
    processed = {}
    filename_map = {}
    
    if not os.path.exists(log_file):
        return processed, filename_map

    logger.info(f"Loading previous scan data from {log_file}...")
    try:
        with open(log_file, 'r', encoding='utf-8', newline='') as f:
            reader = csv.DictReader(f)
            for row in reader:
                try:
                    # Load ALL entries to find successes
                    file_path = row.get('File')
                    if file_path:
                        clean_path = file_path.strip()
                        new_status = row.get('Status')
                        
                        # Prioritize SUCCESS entries over FAILURES.
                        # Check Path Map
                        if clean_path in processed:
                            existing_status = processed[clean_path].get('Status')
                            if existing_status == 'SUCCESS' and new_status != 'SUCCESS':
                                pass # Keep existing success
                            else:
                                processed[clean_path] = row
                        else:
                            processed[clean_path] = row
                            
                        # Update Filename Map (only if SUCCESS, to act as a strong fallback)
                        if new_status == 'SUCCESS':
                            filename = os.path.basename(clean_path)
                            filename_map[filename] = row
                            
                except Exception:
                    pass 
    except Exception as e:
        logger.error(f"Could not read log file {log_file} for resuming: {e}")
    
    logger.info(f"Found {len(processed)} entries in log.")
    return processed, filename_map


def find_movie_files(input_dir: str):
    for root, _, files in os.walk(input_dir):
        for file in files:
            if file.lower().endswith(VIDEO_EXTENSIONS):
                yield os.path.join(root, file)


def _parse_ebur128_stderr(stderr_output: str) -> Optional[Dict[str, float]]:
    """Helper to parse the stderr output from ffmpeg ebur128 filter."""
    patterns = {
        'i':      re.compile(r"Integrated loudness:\s+I:\s*(-?[\d\.]+)\s+LUFS"),
        'thresh': re.compile(r"Integrated loudness:\s+I:.*?\s+Threshold:\s*(-?[\d\.]+)\s+LUFS"),
        'lra':    re.compile(r"Loudness range:\s+LRA:\s*(-?[\d\.]+)\s+LU"),
        'tp':     re.compile(r"True peak:\s+Peak:\s*(-?[\d\.]+)\s+dBFS")
    }
    
    loudness_data = {}
    missing_keys = []
    
    for key, pattern in patterns.items():
        match = pattern.search(stderr_output)
        if match:
            loudness_data[key] = float(match.group(1))
        else:
            missing_keys.append(key)
            
    if missing_keys:
        return None
    return loudness_data


def analyze_loudness(file_path: str, num_threads: int) -> Tuple[str, Dict[str, Any] or str, float]:
    start_time = time.time()
    
    # --- Attempt 1: The "Pure" Command (v2.6.0 standard) ---
    cmd_pure = [
        'ffmpeg', '-nostdin', '-i', file_path,
        '-threads', str(num_threads),
        '-map', '0:a:0',
        '-af', 'ebur128=peak=true',
        '-f', 'null', '-'
    ]
    
    try:
        # Run Pure Command
        result = subprocess.run(cmd_pure, capture_output=True, text=True, encoding='utf-8', timeout=1200)
        scan_time = time.time() - start_time
        
        loudness_data = _parse_ebur128_stderr(result.stderr)
        
        if loudness_data:
            return 'SUCCESS', loudness_data, scan_time
        
        # Check for crash requiring fallback
        err_msg = result.stderr if result.stderr else ""
        needs_fallback = False
        
        if "Invalid argument" in err_msg or "Unsupported channel layout" in err_msg:
            needs_fallback = True
        elif "Stream map '0:a:0' matches no streams" in err_msg:
            return 'ERROR', 'No audio stream found at 0:a:0', scan_time
            
        # --- Attempt 2: Compatibility Mode (Fallback) ---
        if needs_fallback:
            cmd_safe = [
                'ffmpeg', '-nostdin', '-i', file_path,
                '-threads', str(num_threads),
                '-map', '0:a:0',
                '-af', 'ebur128=peak=true,aformat=channel_layouts=stereo:sample_fmts=s16',
                '-f', 'null', '-'
            ]
            
            result_safe = subprocess.run(cmd_safe, capture_output=True, text=True, encoding='utf-8', timeout=1200)
            scan_time = time.time() - start_time 
            
            loudness_data_safe = _parse_ebur128_stderr(result_safe.stderr)
            
            if loudness_data_safe:
                return 'SUCCESS', loudness_data_safe, scan_time
            
            last_lines = "\n".join(result_safe.stderr.strip().split('\n')[-5:])
            return 'ERROR', f'Analysis failed (including compatibility fallback). FFmpeg said:\n{last_lines}', scan_time

        else:
            last_lines = "\n".join(err_msg.strip().split('\n')[-5:])
            return 'ERROR', f'Could not parse output. FFmpeg said:\n{last_lines}', scan_time

    except subprocess.TimeoutExpired:
        return 'ERROR', 'ffmpeg process timed out (20 minutes)', time.time() - start_time
    except FileNotFoundError:
        return 'ERROR', 'ffmpeg command not found in PATH', 0.0
    except Exception as e:
        return 'ERROR', f'An unexpected error occurred: {e}', time.time() - start_time


def process_file_worker(args_tuple: Tuple[str, int]) -> Tuple[str, str, str, Dict[str, Any] or str, float]:
    file_path, num_threads = args_tuple
    try:
        file_size_bytes = os.path.getsize(file_path)
        file_size_str = format_size(file_size_bytes)
    except OSError as e:
        file_size_str = f"Error ({e.strerror})"
    
    print(f"STARTING | File: {os.path.basename(file_path)} | Size: {file_size_str}")
    sys.stdout.flush()

    status, data, scan_time = analyze_loudness(file_path, num_threads)
    return (file_path, file_size_str, status, data, scan_time)


def trigger_hvec_normalization(file_path: str):
    """
    Calls ,hvec to normalize the file.
    Command: ,hvec -i <file> --remux --normalize -S -D
    """
    print(f"\n{GREEN}--- Initiating Normalization ---{RESET}")
    
    if not shutil.which(',hvec'):
         print(f"{RED}Error: Command ',hvec' not found in PATH.{RESET}")
         return

    cmd = [',hvec', '-i', file_path, '--remux', '--normalize', '-S', '-D']
    
    try:
        subprocess.run(cmd, check=True)
    except subprocess.CalledProcessError:
        print(f"\n{RED}Error: ,hvec normalization failed.{RESET}")
    except KeyboardInterrupt:
        print(f"\n{RED}Normalization cancelled.{RESET}")


def main():
    print(f",audit_loudness v{__version__}")

    parser = argparse.ArgumentParser(description="Audit video file loudness.")
    parser.add_argument('-i', '--input', required=True, help='The root directory to scan or a single file to audit.')
    parser.add_argument('-L', '--log', help=f'Specify a custom path/name for the log file (default: {DEFAULT_LOG_FILENAME})')
    parser.add_argument('-T', '--threads', type=int, default=DEFAULT_WORKERS, help=f'Number of parallel processes (default: {DEFAULT_WORKERS})')
    parser.add_argument('--no-nice', action='store_true', help='Disable auto-priority adjustment.')
    parser.add_argument('-v', '--version', action='version', version=f'%(prog)s {__version__}', help="Show script's version number and exit")
    
    args = parser.parse_args()

    if args.log:
        log_file_path = os.path.abspath(args.log)
    else:
        log_file_path = DEFAULT_LOG_FILENAME

    log_dir = os.path.dirname(log_file_path)
    if log_dir and not os.path.exists(log_dir):
        try:
            os.makedirs(log_dir)
            print(f"Info: Created log directory: {log_dir}")
        except OSError as e:
            print(f"Error: Could not create log directory {log_dir}. {e}", file=sys.stderr)
            sys.exit(1)

    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(logging.Formatter('%(message)s'))
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    logger.addHandler(console_handler)
    
    is_new_file = not os.path.exists(log_file_path) or os.path.getsize(log_file_path) == 0
    if is_new_file:
        try:
            with open(log_file_path, 'w', encoding='utf-8', newline='') as f:
                csv_writer = csv.writer(f)
                csv_writer.writerow(LOG_HEADER)
        except OSError as e:
            logger.error(f"FATAL: Could not write header to log file: {e}")
            sys.exit(1)

    if not os.path.exists(args.input):
        logger.error(f"Error: Input path not found: {args.input}")
        sys.exit(1)

    processed_files_map, processed_filenames_map = load_processed_files(log_file_path, logger)
    
    processed_count = 0
    error_count = 0
    pool = None
    all_files = []
    
    file_needing_normalization = None

    stats_increase_count = 0
    stats_increase_sum = 0.0
    stats_decrease_count = 0
    stats_decrease_sum = 0.0

    try:
        if os.path.isfile(args.input):
            all_files = [args.input]
            logger.info("Processing single file.")
        elif os.path.isdir(args.input):
            logger.info(f"Scanning directory: {args.input}")
            all_files = list(find_movie_files(args.input))
        else:
            logger.error(f"Error: Input path is not a valid file or directory: {args.input}")
            sys.exit(1)

        total_files = len(all_files)
        files_to_process = []
        skipped_count = 0
        
        for f in all_files:
            abs_path = os.path.abspath(f)
            cached_data = processed_files_map.get(abs_path)
            
            if not cached_data:
                cached_data = processed_filenames_map.get(os.path.basename(abs_path))
            
            should_scan = True
            
            if cached_data:
                cached_status = cached_data.get('Status', 'UNKNOWN')
                
                if cached_status == 'SUCCESS':
                    should_scan = False
                    skipped_count += 1
                    
                    if len(all_files) == 1:
                        try:
                            cached_i = float(cached_data.get('CurrentLoudness', 0.0))
                            cached_adj = float(cached_data.get('Adjustment_dB', 0.0))
                            cached_size_str = cached_data.get('Size', 'N/A')
                            timestamp = cached_data.get('Timestamp', 'Unknown')
                            
                            # v2.9.4: Smart Normalization Detection
                            # If adjustment is effectively 0, file is already normalized.
                            if abs(cached_adj) < 0.5:
                                status_label = f"{GREEN}[NORMALIZED]{RESET}"
                                adj_color = GREEN
                                file_needing_normalization = None # Explicitly prevent prompting
                            else:
                                status_label = f"{YELLOW}[ADJUSTMENT AVAILABLE]{RESET}"
                                adj_color = GREEN if cached_adj >= 0 else RED
                                file_needing_normalization = abs_path

                            logger.info(f"\n[LOG] {os.path.basename(abs_path)}")
                            logger.info(f"      Analyzed: {timestamp}")
                            logger.info(f"      Size: {cached_size_str} | Status: {status_label}")
                            logger.info(f"      Loudness: {cached_i:.1f} LUFS | Gain: {adj_color}{cached_adj:+.2f} dB{RESET}")
                            
                        except ValueError:
                            pass 
                
                else:
                    should_scan = True
                    if len(all_files) == 1:
                        logger.info(f"{YELLOW}[Log Entry Found (Status: FAILURE) - Re-scanning to retrieve data]{RESET}")
            
            if should_scan:
                files_to_process.append(abs_path)

        logger.info(f"Found {total_files} total video files.")
        if skipped_count > 0:
            logger.info(f"Skipping {skipped_count} files (already in log).")
        
        if not files_to_process:
            logger.info("No new or modified files to process.")
        
        else:
            if len(files_to_process) == 1 and total_files == 1:
                num_workers = 1
                logger.info(f"Processing 1 file (using {args.threads} ffmpeg threads)...")
                result = process_file_worker((files_to_process[0], args.threads))
                results_iterator = [result]
            else:
                num_workers = max(1, min(args.threads, len(files_to_process)))
                logger.info(f"Processing {len(files_to_process)} files using {num_workers} parallel workers...")
                ffmpeg_threads = 1
                tasks = [(f, ffmpeg_threads) for f in files_to_process]
                pool = multiprocessing.Pool(processes=num_workers)
                results_iterator = pool.imap_unordered(process_file_worker, tasks)
        
            with open(log_file_path, 'a', encoding='utf-8', newline='') as data_file:
                csv_writer = csv.writer(data_file)
                
                for file_path, file_size_str, status, data, scan_time in tqdm(results_iterator, total=len(files_to_process), desc="Analyzing", unit="file", ncols=100):
                    timestamp = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())
                    
                    if status == 'SUCCESS':
                        loudness_data = data
                        input_i = loudness_data['i']
                        adjustment = TARGET_LOUDNESS - input_i
                        
                        if adjustment >= 0:
                            action = "Increase"
                            color = GREEN
                            stats_increase_count += 1
                            stats_increase_sum += adjustment
                        else:
                            action = "Decrease"
                            color = RED
                            stats_decrease_count += 1
                            stats_decrease_sum += adjustment

                        tqdm.write(f"  > {os.path.basename(file_path)}")
                        tqdm.write(f"    Loudness: {input_i:.1f} LUFS | Target: {TARGET_LOUDNESS} LUFS | {color}{action} Gain: {adjustment:+.2f} dB{RESET}")
                        
                        if len(all_files) == 1 and abs(adjustment) >= 0.5:
                            file_needing_normalization = file_path

                        log_row = [
                            timestamp, 'SUCCESS', file_path, file_size_str,
                            f"{scan_time:.2f}s", f"{input_i:.2f}", f"{adjustment:+.2f}",
                            f"{loudness_data['tp']:.2f}", f"{loudness_data['lra']:.2f}",
                            f"{loudness_data['thresh']:.2f}", ""
                        ]
                        csv_writer.writerow(log_row)
                        processed_count += 1
                    else:
                        error_message = str(data).strip().replace("\n", " ")
                        tqdm.write(f"  > {os.path.basename(file_path)}")
                        tqdm.write(f"    {RED}Analysis Failed: {error_message}{RESET}")
                        
                        log_row = [
                            timestamp, 'FAILURE', file_path, file_size_str,
                            f"{scan_time:.2f}s", "", "", "", "", "", error_message
                        ]
                        csv_writer.writerow(log_row)
                        error_count += 1
            
            if pool:
                pool.close()
                pool.join()

    except KeyboardInterrupt:
        logger.info("\n\n--- User interrupted ---")
        if pool:
            pool.terminate()
            pool.join()
    
    except Exception as e:
        logger.error(f"\n\n--- A fatal error occurred ---")
        logger.error(e)
        if pool:
            pool.terminate()
            pool.join()

    finally:
        logger.info("\n--- Audit Complete ---")
        logger.info(f"Successfully processed: {processed_count}")
        logger.info(f"Skipped (already in log): {skipped_count}")
        logger.info(f"Errors: {error_count}")
        
        if file_needing_normalization and error_count == 0:
            logger.info(f"\n{YELLOW}Normalization Available:{RESET} This file can be adjusted to the target loudness.")
            
            timeout = PROMPT_TIMEOUT_SECONDS
            user_input = None
            
            try:
                for i in range(timeout, 0, -1):
                    sys.stdout.write(f"\rApply normalization now using ,hvec? (Replaces original file) [y/N] ({i}s) ")
                    sys.stdout.flush()
                    
                    rlist, _, _ = select.select([sys.stdin], [], [], 1)
                    if rlist:
                        user_input = sys.stdin.readline().strip().lower()
                        break
                
                if user_input is None:
                     sys.stdout.write(f"\rApply normalization now using ,hvec? (Replaces original file) [y/N] (Timeout - Defaulting to No)\n")
                     response = 'n'
                else:
                     sys.stdout.write("\n")
                     response = user_input

                if response == 'y':
                    trigger_hvec_normalization(file_needing_normalization)
            except KeyboardInterrupt:
                print("\nCancelled.")

if __name__ == "__main__":
    main()
